{"cells":[{"cell_type":"code","execution_count":null,"id":"ab3d22d3","metadata":{"id":"ab3d22d3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"id":"2562b0f0","metadata":{"id":"2562b0f0"},"outputs":[],"source":["class ID3Dat:\n","    \n","    # Membuat fungsi untuk model klasifikasi dari data latih dan data uji yang telah ditetapkan\n","    def fit(self, input, output):\n","        data = input.copy()\n","        data[output.name] = output\n","        self.tree = self.decision_tree(data, data, input.columns, output.name)\n","\n","    # Membuat fungsi predict\n","    def predict(self, input):\n","        #konversikan data input ke variabel samples\n","        samples = input.to_dict(orient='records')\n","        predictions = []\n","\n","        #membuat prediksi untuk setiap sample\n","        for sample in samples:\n","            predictions.append(self.make_prediction(sample, self.tree, 1.0))\n","\n","        return predictions\n","\n","    # Mengaplikasikan rumus entropy\n","    def entropy(self, attribute_column):\n","        \n","        # menemukan nilai unik\n","        values, counts = np.unique(attribute_column, return_counts=True)\n","\n","        # menghitung nilai entropy sesuai dengan nilai unik yang ditemukan\n","        entropy_list = []\n","\n","        for i in range(len(values)):\n","            probability = counts[i]/np.sum(counts)\n","            entropy_list.append(-probability*np.log2(probability))\n","\n","        # menjumlahkan setiap nilai entropy\n","        total_entropy = np.sum(entropy_list)\n","\n","        return total_entropy\n","\n","    def information_gain(self, data, feature_attribute_name, target_attribute_name):\n","        \n","        # menemukan total entropy\n","        total_entropy = self.entropy(data[target_attribute_name])\n","\n","        # menemukan nilai unik dan menghitung masing-masing frekuensi untuk dipisahkan\n","        values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n","\n","        # menghitung total entropy setiap subset\n","        weighted_entropy_list = []\n","\n","        for i in range(len(values)):\n","            subset_probability = counts[i]/np.sum(counts)\n","            subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n","            weighted_entropy_list.append(subset_probability*subset_entropy)\n","\n","        total_weighted_entropy = np.sum(weighted_entropy_list)\n","\n","        # menghitung information gain\n","        information_gain = total_entropy - total_weighted_entropy\n","\n","        return information_gain\n","\n","    # model decision tree dengan algoritma Id3\n","    def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n","        \n","        # jika data bersih, kembalikan nilai unik setiap subset\n","        unique_classes = np.unique(data[target_attribute_name])\n","        if len(unique_classes) <= 1:\n","            return unique_classes[0]\n","        # jika subset kosong, kembalikan nilai unik ke data originalnya\n","        elif len(data) == 0:\n","            majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n","            return np.unique(original_data[target_attribute_name])[majority_class_index]\n","        # jika data set tidak mengandung nilai untuk dilatih, kembalikan nilai ke parent node class\n","        elif len(feature_attribute_names) == 0:\n","            return parent_node_class\n","        # jika tidak terjadi seluruh syarat diatas\n","        else:\n","            majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n","            parent_node_class = unique_classes[majority_class_index]\n","\n","            ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n","            best_feature_index = np.argmax(ig_values)\n","            best_feature = feature_attribute_names[best_feature_index]\n","\n","            tree = {best_feature: {}}\n","\n","            feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n","\n","            parent_attribute_values = np.unique(data[best_feature])\n","            for value in parent_attribute_values:\n","                sub_data = data.where(data[best_feature] == value).dropna()\n","\n","                subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n","\n","                tree[best_feature][value] = subtree\n","\n","            return tree\n","\n","    # membuat fungsi untukk prediksi\n","    def make_prediction(self, sample, tree, default=1):\n","        for attribute in list(sample.keys()):\n","          if attribute in list(tree.keys()):\n","            try:\n","                result = tree[attribute][sample[attribute]]\n","            except:\n","                return default\n","\n","            result = tree[attribute][sample[attribute]]\n","\n","            if isinstance(result, dict):\n","                return self.make_prediction(sample, result)\n","            else:\n","                return result"]},{"cell_type":"code","execution_count":null,"id":"75b8c126","metadata":{"id":"75b8c126"},"outputs":[],"source":["df = pd.read_csv('D:\\~~COLLAGE~~\\SEMESTER 6\\APM\\P2\\Wine.csv')\n","\n","# organize data into input and output\n","X = df.drop(columns=\"Class\")\n","y = df[\"Class\"]"]},{"cell_type":"code","execution_count":null,"id":"b3ad5292","metadata":{"id":"b3ad5292","outputId":"ce7d17fe-3f38-4b6b-bb4d-868459a28fa6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Alcohol</th>\n","      <th>Malic acid</th>\n","      <th>Ash</th>\n","      <th>Alcalinity of ash</th>\n","      <th>Magnesium</th>\n","      <th>Total phenols</th>\n","      <th>Flavanoids</th>\n","      <th>Nonflavanoid phenols</th>\n","      <th>Proanthocyanins</th>\n","      <th>Color intensity</th>\n","      <th>Hue</th>\n","      <th>OD280/OD315 of diluted wines</th>\n","      <th>Proline</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14.23</td>\n","      <td>1.71</td>\n","      <td>2.43</td>\n","      <td>15.6</td>\n","      <td>127</td>\n","      <td>2.80</td>\n","      <td>3.06</td>\n","      <td>0.28</td>\n","      <td>2.29</td>\n","      <td>5.64</td>\n","      <td>1.04</td>\n","      <td>3.92</td>\n","      <td>1065</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13.20</td>\n","      <td>1.78</td>\n","      <td>2.14</td>\n","      <td>11.2</td>\n","      <td>100</td>\n","      <td>2.65</td>\n","      <td>2.76</td>\n","      <td>0.26</td>\n","      <td>1.28</td>\n","      <td>4.38</td>\n","      <td>1.05</td>\n","      <td>3.40</td>\n","      <td>1050</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13.16</td>\n","      <td>2.36</td>\n","      <td>2.67</td>\n","      <td>18.6</td>\n","      <td>101</td>\n","      <td>2.80</td>\n","      <td>3.24</td>\n","      <td>0.30</td>\n","      <td>2.81</td>\n","      <td>5.68</td>\n","      <td>1.03</td>\n","      <td>3.17</td>\n","      <td>1185</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14.37</td>\n","      <td>1.95</td>\n","      <td>2.50</td>\n","      <td>16.8</td>\n","      <td>113</td>\n","      <td>3.85</td>\n","      <td>3.49</td>\n","      <td>0.24</td>\n","      <td>2.18</td>\n","      <td>7.80</td>\n","      <td>0.86</td>\n","      <td>3.45</td>\n","      <td>1480</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13.24</td>\n","      <td>2.59</td>\n","      <td>2.87</td>\n","      <td>21.0</td>\n","      <td>118</td>\n","      <td>2.80</td>\n","      <td>2.69</td>\n","      <td>0.39</td>\n","      <td>1.82</td>\n","      <td>4.32</td>\n","      <td>1.04</td>\n","      <td>2.93</td>\n","      <td>735</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>173</th>\n","      <td>13.71</td>\n","      <td>5.65</td>\n","      <td>2.45</td>\n","      <td>20.5</td>\n","      <td>95</td>\n","      <td>1.68</td>\n","      <td>0.61</td>\n","      <td>0.52</td>\n","      <td>1.06</td>\n","      <td>7.70</td>\n","      <td>0.64</td>\n","      <td>1.74</td>\n","      <td>740</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>13.40</td>\n","      <td>3.91</td>\n","      <td>2.48</td>\n","      <td>23.0</td>\n","      <td>102</td>\n","      <td>1.80</td>\n","      <td>0.75</td>\n","      <td>0.43</td>\n","      <td>1.41</td>\n","      <td>7.30</td>\n","      <td>0.70</td>\n","      <td>1.56</td>\n","      <td>750</td>\n","    </tr>\n","    <tr>\n","      <th>175</th>\n","      <td>13.27</td>\n","      <td>4.28</td>\n","      <td>2.26</td>\n","      <td>20.0</td>\n","      <td>120</td>\n","      <td>1.59</td>\n","      <td>0.69</td>\n","      <td>0.43</td>\n","      <td>1.35</td>\n","      <td>10.20</td>\n","      <td>0.59</td>\n","      <td>1.56</td>\n","      <td>835</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>13.17</td>\n","      <td>2.59</td>\n","      <td>2.37</td>\n","      <td>20.0</td>\n","      <td>120</td>\n","      <td>1.65</td>\n","      <td>0.68</td>\n","      <td>0.53</td>\n","      <td>1.46</td>\n","      <td>9.30</td>\n","      <td>0.60</td>\n","      <td>1.62</td>\n","      <td>840</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>14.13</td>\n","      <td>4.10</td>\n","      <td>2.74</td>\n","      <td>24.5</td>\n","      <td>96</td>\n","      <td>2.05</td>\n","      <td>0.76</td>\n","      <td>0.56</td>\n","      <td>1.35</td>\n","      <td>9.20</td>\n","      <td>0.61</td>\n","      <td>1.60</td>\n","      <td>560</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>178 rows Ã— 13 columns</p>\n","</div>"],"text/plain":["     Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n","0      14.23        1.71  2.43               15.6        127           2.80   \n","1      13.20        1.78  2.14               11.2        100           2.65   \n","2      13.16        2.36  2.67               18.6        101           2.80   \n","3      14.37        1.95  2.50               16.8        113           3.85   \n","4      13.24        2.59  2.87               21.0        118           2.80   \n","..       ...         ...   ...                ...        ...            ...   \n","173    13.71        5.65  2.45               20.5         95           1.68   \n","174    13.40        3.91  2.48               23.0        102           1.80   \n","175    13.27        4.28  2.26               20.0        120           1.59   \n","176    13.17        2.59  2.37               20.0        120           1.65   \n","177    14.13        4.10  2.74               24.5         96           2.05   \n","\n","     Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n","0          3.06                  0.28             2.29             5.64  1.04   \n","1          2.76                  0.26             1.28             4.38  1.05   \n","2          3.24                  0.30             2.81             5.68  1.03   \n","3          3.49                  0.24             2.18             7.80  0.86   \n","4          2.69                  0.39             1.82             4.32  1.04   \n","..          ...                   ...              ...              ...   ...   \n","173        0.61                  0.52             1.06             7.70  0.64   \n","174        0.75                  0.43             1.41             7.30  0.70   \n","175        0.69                  0.43             1.35            10.20  0.59   \n","176        0.68                  0.53             1.46             9.30  0.60   \n","177        0.76                  0.56             1.35             9.20  0.61   \n","\n","     OD280/OD315 of diluted wines  Proline  \n","0                            3.92     1065  \n","1                            3.40     1050  \n","2                            3.17     1185  \n","3                            3.45     1480  \n","4                            2.93      735  \n","..                            ...      ...  \n","173                          1.74      740  \n","174                          1.56      750  \n","175                          1.56      835  \n","176                          1.62      840  \n","177                          1.60      560  \n","\n","[178 rows x 13 columns]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":null,"id":"daaa0d50","metadata":{"id":"daaa0d50","outputId":"19aa15bc-1fb4-456a-c8de-217e3fa57070"},"outputs":[{"data":{"text/plain":["0      1\n","1      1\n","2      1\n","3      1\n","4      1\n","      ..\n","173    3\n","174    3\n","175    3\n","176    3\n","177    3\n","Name: Class, Length: 178, dtype: int64"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"id":"20a11568","metadata":{"id":"20a11568","outputId":"85d9f48f-6d5a-4dd3-944b-1324ab9731f6"},"outputs":[{"data":{"text/plain":["0.6111111111111112"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n","\n","# initialize and fit model\n","model = ID3Dat()\n","model.fit(X_train, y_train)\n","\n","# return accuracy score\n","y_pred = model.predict(X_test)\n","accuracy_score(y_test, y_pred)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"id3.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}